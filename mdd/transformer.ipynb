{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "228eb205-7cfb-476b-8d89-2e92770df6c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, col\n",
    "from mdd.utils import DecoratorUtil, DeltaTableUtil, FunctionUtil\n",
    "from mdd.metadata import Metadata\n",
    "from mdd.datareader import DeltaTableReader\n",
    "from mdd.datawriter import DeltaTableWriter\n",
    "from pyspark.sql import SparkSession\n",
    "import logging\n",
    "\n",
    "\n",
    "@DecoratorUtil.add_logger()\n",
    "class TransformDataFlow:\n",
    "    logger: logging.Logger\n",
    "    \n",
    "    def __init__(self, spark: SparkSession, metadata_yml: str):\n",
    "        self.spark = spark\n",
    "        self.metadata = Metadata(metadata_yml, False)\n",
    "        self.debug = self.metadata.get(\"debug\")\n",
    "        self.active = self.metadata.get(\"active\")\n",
    "        self.source_name = DeltaTableUtil.qualify_table_name(self.metadata.get(\"reader\", \"source_name\"))\n",
    "        self.sink_name = DeltaTableUtil.qualify_table_name(self.metadata.get(\"writer\", \"sink_name\"))\n",
    "\n",
    "        dataflow_type = self.metadata.get(\"dataflow_type\")\n",
    "        if dataflow_type != \"transform\":\n",
    "            message = (\n",
    "                f\"Invalid dataflow type: {dataflow_type}, it should be 'transform'\"\n",
    "            )\n",
    "            self.logger.error(message)\n",
    "            raise Exception(message)\n",
    "\n",
    "    @DecoratorUtil.log_function()\n",
    "    def read(self) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Reads data from the specified Delta table using the provided configuration.\n",
    "\n",
    "        :return: Spark DataFrame containing the read data\n",
    "        \"\"\"\n",
    "        mode = self.metadata.get(\"sync_options\", \"mode\")\n",
    "        backfill_days = self.metadata.get(\"sync_options\", \"backfill_days\")\n",
    "\n",
    "        # build the config for data reader\n",
    "        config = {}\n",
    "        config[\"source_name\"] = self.source_name\n",
    "        config[\"mode\"] = mode\n",
    "        config[\"backfill_days\"] = backfill_days\n",
    "        if mode == \"full\":\n",
    "            # get the max _source_timestamp from the target table\n",
    "            max_source_timestamp = DeltaTableUtil.get_max_column_value(\n",
    "                self.spark, self.sink_name, \"_source_timestamp\"\n",
    "            )\n",
    "            config[\"full_max_processed_timestamp\"] = max_source_timestamp\n",
    "        elif mode == \"incremental\":\n",
    "            max_source_commit_version = DeltaTableUtil.get_max_column_value(\n",
    "                self.spark,\n",
    "                \"mdd.table_control\",\n",
    "                \"source_commit_version\",\n",
    "                f\"table_name = '{self.sink_name}' and source_name = '{self.source_name}'\",\n",
    "            )\n",
    "            config[\"incremental_max_processed_version\"] = max_source_commit_version\n",
    "        else:\n",
    "            message = f\"Invalid mode: {mode}\"\n",
    "            self.logger.error(message)\n",
    "            raise ValueError(message)\n",
    "\n",
    "        if self.debug:\n",
    "            self.logger.info(f\"Config: {config}\")\n",
    "\n",
    "        reader = DeltaTableReader(spark=self.spark, config=config, debug=self.debug)\n",
    "        df = reader.read()\n",
    "\n",
    "        #df = self.deduplicate(df)\n",
    "        return df\n",
    "\n",
    "    @DecoratorUtil.log_function()\n",
    "    def deduplicate(self, df: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Deduplicates the DataFrame by keeping only one record per primary key,\n",
    "        using composite deduplication columns with optional sort direction.\n",
    "\n",
    "        :param df: Input Spark DataFrame\n",
    "        :param primary_key: Comma-separated list of primary key columns (e.g., \"id,sub_id\")\n",
    "        :param deduplication_columns: String defining columns and sort directions\n",
    "            (e.g., \"event_time desc, event_sequence\", defaults to asc if not specified)\n",
    "        :return: Deduplicated DataFrame\n",
    "        \"\"\"\n",
    "\n",
    "        primary_key = self.metadata.get(\"reader\", \"source_primarykey\")\n",
    "        deduplication_columns = self.metadata.get(\n",
    "            \"reader\", \"source_deduplication_columns\"\n",
    "        )\n",
    "\n",
    "        if not primary_key or not deduplication_columns:\n",
    "            message = \"source_primarykey and source_deduplication_columns are required\"\n",
    "            self.logger.error(message)\n",
    "            raise ValueError(message)\n",
    "\n",
    "        primary_keys = FunctionUtil.string_to_list(primary_key)\n",
    "        try:\n",
    "            # Parse deduplication column string\n",
    "            sort_exprs = []\n",
    "            for entry in deduplication_columns.split(\",\"):\n",
    "                parts = entry.strip().split()\n",
    "                if len(parts) == 1:\n",
    "                    col_name, direction = parts[0], \"asc\"\n",
    "                elif len(parts) == 2:\n",
    "                    col_name, direction = parts[0], parts[1].lower()\n",
    "                else:\n",
    "                    raise ValueError(\n",
    "                        f\"Invalid format for deduplication column: '{entry.strip()}'\"\n",
    "                    )\n",
    "\n",
    "                if direction == \"desc\":\n",
    "                    sort_exprs.append(col(col_name).desc())\n",
    "                elif direction == \"asc\":\n",
    "                    sort_exprs.append(col(col_name).asc())\n",
    "                else:\n",
    "                    raise ValueError(\n",
    "                        f\"Unsupported sort order '{direction}' for column '{col_name}'\"\n",
    "                    )\n",
    "\n",
    "            window_spec = Window.partitionBy(*primary_keys).orderBy(*sort_exprs)\n",
    "            df_deduplicated = (\n",
    "                df.withColumn(\"_row_number\", row_number().over(window_spec))\n",
    "                .filter(col(\"_row_number\") == 1)\n",
    "                .drop(\"_row_number\")\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(\"Deduplication failed with exception: %s\")\n",
    "            raise\n",
    "\n",
    "        return df_deduplicated\n",
    "\n",
    "    @DecoratorUtil.log_function()\n",
    "    def transform(self, df: DataFrame) -> DataFrame:\n",
    "        return df\n",
    "\n",
    "    @DecoratorUtil.log_function()\n",
    "    def write_stream(self, df: DataFrame):\n",
    "        config_writer = self.metadata.get(\"writer\")\n",
    "        writer = DeltaTableWriter(self.spark, df, config_writer, self.debug)\n",
    "        query = writer.write_stream()\n",
    "        query.awaitTermination()\n",
    "\n",
    "    @DecoratorUtil.log_function()\n",
    "    def run(self):\n",
    "        self.logger.info(\"Transform start ...\")\n",
    "\n",
    "        self.logger.info(\"Read start...\")\n",
    "        df = self.read()\n",
    "        self.logger.info(\"Read end...\")\n",
    "\n",
    "        self.logger.info(\"Deduplicate start...\")\n",
    "        source_primarykey = self.metadata.get(\"source_primarykey\")\n",
    "        source_deduplication_columns = self.metadata.get(\"source_deduplication_columns\")\n",
    "        df = self.deduplicate(df, source_primarykey, source_deduplication_columns)\n",
    "        self.logger.info(\"Deduplicate end...\")\n",
    "\n",
    "        self.logger.info(\"Deduplicate start...\")\n",
    "        df = self.transform(df)\n",
    "        self.logger.info(\"Deduplicate end...\")\n",
    "\n",
    "        self.logger.info(\"Write start...\")\n",
    "        self.write_stream(df)\n",
    "        self.logger.info(\"Write end...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60cddfa7-bc65-43d3-9bf1-a40a965a9fa5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import logging\n",
    "import datetime\n",
    "from mdd.logger import *\n",
    "#from mdd.transformer import *\n",
    "\n",
    "log_folder = \"mdd_test\"\n",
    "log_file_name = \"test_transformer\"\n",
    "log_timestamp = datetime.datetime.now()\n",
    "debug = False\n",
    "Logger.init(log_folder, log_file_name, log_timestamp, debug)\n",
    "\n",
    "try:\n",
    "    metadata_yml = \"transform/gold_fact_combinedcards.yml\"\n",
    "    dataflow = TransformDataFlow(spark, metadata_yml)\n",
    "    df_read = dataflow.read()\n",
    "    display(df_read)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "finally:\n",
    "    logging.shutdown()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "370ff687-64bb-4ae6-ab7a-55c9db998336",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_read.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "159bb849-71a0-460e-a507-74a926571232",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select count(*) from lakehouse.bronze.paytronix_mid352_combinedcards;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3e73425-2116-4241-b926-f854d747a213",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "drop_columns = [\"_corrupt_record\", \"_rescued_data\", \"_mode\", \"_change_type\", \"_commit_version\", \"_commit_timestamp\"]\n",
    "\n",
    "from mdd.helper.deltatable import DeltaTableUtils as dtu\n",
    "df_transformed = DeltaTableUtil.safe_drop_columns(df_read, drop_columns)\n",
    "display(df_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b9c7653-16bc-447a-b88d-a33d8a97a5b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_transformed.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e440f65c-61a6-4ea0-82ef-d41d4f989d88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataflow.write_stream(df_transformed)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7830954160519784,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "transformer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
